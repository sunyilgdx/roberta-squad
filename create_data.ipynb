{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-v2.0.json                          : 19035 contexts\n",
      "2500 features (182 no ans) extracted (time: 1.38 s)\n",
      "5000 features (357 no ans) extracted (time: 1.58 s)\n",
      "7500 features (387 no ans) extracted (time: 1.75 s)\n",
      "10000 features (1757 no ans) extracted (time: 1.91 s)\n",
      "12500 features (2797 no ans) extracted (time: 2.05 s)\n",
      "15000 features (3694 no ans) extracted (time: 2.19 s)\n",
      "17500 features (4409 no ans) extracted (time: 2.33 s)\n",
      "20000 features (5503 no ans) extracted (time: 2.48 s)\n",
      "22500 features (6196 no ans) extracted (time: 2.62 s)\n",
      "25000 features (6363 no ans) extracted (time: 2.79 s)\n",
      "27500 features (6859 no ans) extracted (time: 2.95 s)\n",
      "30000 features (6974 no ans) extracted (time: 3.16 s)\n",
      "32500 features (7947 no ans) extracted (time: 3.34 s)\n",
      "35000 features (9153 no ans) extracted (time: 3.50 s)\n",
      "37500 features (10411 no ans) extracted (time: 3.67 s)\n",
      "40000 features (11232 no ans) extracted (time: 3.87 s)\n",
      "42500 features (11918 no ans) extracted (time: 4.24 s)\n",
      "45000 features (13009 no ans) extracted (time: 4.48 s)\n",
      "47500 features (13936 no ans) extracted (time: 4.64 s)\n",
      "50000 features (14790 no ans) extracted (time: 4.80 s)\n",
      "52500 features (15357 no ans) extracted (time: 4.97 s)\n",
      "55000 features (16132 no ans) extracted (time: 5.12 s)\n",
      "57500 features (16836 no ans) extracted (time: 5.30 s)\n",
      "60000 features (17962 no ans) extracted (time: 5.45 s)\n",
      "62500 features (18606 no ans) extracted (time: 5.63 s)\n",
      "65000 features (19802 no ans) extracted (time: 5.79 s)\n",
      "67500 features (20870 no ans) extracted (time: 5.96 s)\n",
      "70000 features (21822 no ans) extracted (time: 6.13 s)\n",
      "72500 features (22814 no ans) extracted (time: 6.31 s)\n",
      "75000 features (23584 no ans) extracted (time: 6.47 s)\n",
      "77500 features (24170 no ans) extracted (time: 6.65 s)\n",
      "80000 features (25365 no ans) extracted (time: 6.81 s)\n",
      "82500 features (26384 no ans) extracted (time: 6.96 s)\n",
      "85000 features (27171 no ans) extracted (time: 7.12 s)\n",
      "87500 features (28265 no ans) extracted (time: 7.27 s)\n",
      "90000 features (29477 no ans) extracted (time: 7.41 s)\n",
      "92500 features (30111 no ans) extracted (time: 7.59 s)\n",
      "95000 features (31356 no ans) extracted (time: 7.75 s)\n",
      "97500 features (32452 no ans) extracted (time: 7.92 s)\n",
      "100000 features (33497 no ans) extracted (time: 8.08 s)\n",
      "102500 features (34110 no ans) extracted (time: 8.25 s)\n",
      "105000 features (35102 no ans) extracted (time: 8.42 s)\n",
      "107500 features (35998 no ans) extracted (time: 8.58 s)\n",
      "110000 features (36786 no ans) extracted (time: 8.77 s)\n",
      "112500 features (37389 no ans) extracted (time: 8.96 s)\n",
      "115000 features (38410 no ans) extracted (time: 9.12 s)\n",
      "117500 features (39629 no ans) extracted (time: 9.26 s)\n",
      "120000 features (40351 no ans) extracted (time: 9.42 s)\n",
      "122500 features (41012 no ans) extracted (time: 9.56 s)\n",
      "125000 features (41840 no ans) extracted (time: 9.70 s)\n",
      "127500 features (42622 no ans) extracted (time: 9.84 s)\n",
      "130000 features (43324 no ans) extracted (time: 9.98 s)\n",
      "num has ans / num no ans : 86772 / 43592\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from time import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from torch import nn\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "import torch\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "from tokenizer.roberta import RobertaTokenizer\n",
    "\n",
    "max_seq_length   = 512\n",
    "max_query_length = 128\n",
    "doc_stride       = 128\n",
    "\n",
    "get_tokenizer = lambda: RobertaTokenizer(config_dir='roberta.large')\n",
    "\n",
    "tk = tokenizer =  get_tokenizer()\n",
    "\n",
    "\n",
    "def init():\n",
    "    global tokenizer, tk\n",
    "    import gc\n",
    "    tokenizer = tk = get_tokenizer()\n",
    "    \n",
    "\n",
    "def char_anchors_to_tok_pos(r):\n",
    "    if len(r.char_anchors) == 2:\n",
    "        a,b = r.char_anchors\n",
    "    else:\n",
    "        return 0,0\n",
    "    a = r.char_to_tok_offset[a]\n",
    "    b = r.char_to_tok_offset[b]\n",
    "    while b+1 < len(r.all_doc_tokens) and r.all_text_tokens[b+1] == '':\n",
    "        b += 1\n",
    "        \n",
    "    return a, b\n",
    "\n",
    "def read(dat):\n",
    "    uid, inp, start, end, p_mask, unanswerable = marshal.loads(dat)\n",
    "    inp = np.frombuffer(inp, dtype=np.uint16).astype(np.int32)\n",
    "    p_mask = np.frombuffer(p_mask, dtype=np.bool).astype(np.float32)\n",
    "    return uid, inp, start, end, p_mask, unanswerable\n",
    "\n",
    "def fread(f):\n",
    "    uid, inp, start, end, p_mask, unanswerable = marshal.load(f)\n",
    "    inp = np.frombuffer(inp, dtype=np.uint16).astype(np.int32)\n",
    "    p_mask = np.frombuffer(p_mask, dtype=np.bool).astype(np.float32)\n",
    "    return uid, inp, start, end, p_mask, unanswerable\n",
    "            \n",
    "def data_from_path(train_dir):\n",
    "    index = 0\n",
    "    for fn in glob(train_dir):\n",
    "        with tf.gfile.Open(fn, \"r\") as f:\n",
    "            entries = [e for e in json.load(f)[\"data\"] for e in e['paragraphs']]\n",
    "\n",
    "\n",
    "        print(\"%-40s : %s contexts\"%(fn.split('/')[-1],len(entries)))\n",
    "        for e in entries:\n",
    "            c = e['context']\n",
    "            yield index, c, e['qas']\n",
    "            index += 1\n",
    "\n",
    "\n",
    "def gen(paths):\n",
    "    i = 0\n",
    "    for i,context,qas in data_from_path(paths):\n",
    "        yield i,context, qas\n",
    "        \n",
    "        \n",
    "import marshal\n",
    "def work(ss, debug=False):\n",
    "    \n",
    "    unique_index, \\\n",
    "     context, \\\n",
    "     qas, \\\n",
    "     is_training, \\\n",
    "     return_feature = ss\n",
    "    for q in qas:\n",
    "        q['question'] = q['question']\n",
    "    \n",
    "    rss = tokenizer.merge_cq(context, \n",
    "                             qas,\n",
    "                             max_seq_length = max_seq_length,\n",
    "                             max_query_length = max_query_length,\n",
    "                             doc_stride = doc_stride,\n",
    "                             unique_index=unique_index,\n",
    "                             is_training=is_training,\n",
    "                             debug = debug\n",
    "                           )\n",
    "    o = 0\n",
    "    results = []\n",
    "    for rs in rss:\n",
    "        q = qas[o]\n",
    "        o += 1\n",
    "        for r in rs:\n",
    "            inp = tk.convert_tokens_to_ids(r.all_doc_tokens)\n",
    "            start_position,end_position = char_anchors_to_tok_pos(r)\n",
    "            p_mask = r.p_mask\n",
    "            uid = r.unique_index[0]*1000 + r.unique_index[1]\n",
    "            \n",
    "            no_ans = start_position == 0\n",
    "            \n",
    "            #if no_ans:\n",
    "            #    print(q['answer_text'], '>>', r.all_doc_tokens[start_position:end_position+1])\n",
    "            assert start_position >= 0 and end_position >= 0 and start_position < len(inp) and end_position < len(inp)\n",
    "            assert len(inp) <= max_seq_length\n",
    "            \n",
    "            S, E = start_position, end_position\n",
    "            \n",
    "            record = marshal.dumps(\n",
    "                (\n",
    "                uid,\n",
    "                np.array(inp,dtype=np.uint16).tobytes(),\n",
    "                start_position,\n",
    "                end_position,\n",
    "                np.array(p_mask,dtype=np.bool).tobytes(),\n",
    "                int(no_ans)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            if return_feature:\n",
    "                results.append((record, no_ans,r.serialize()))\n",
    "            else:\n",
    "                results.append((record, no_ans))\n",
    "\n",
    "\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def generate_tfrecord(data_dir,\n",
    "                      write_fn=None, \n",
    "                      is_training=False,\n",
    "                      return_feature=False,\n",
    "                      parallel_process=False,\n",
    "                      debug=False):\n",
    "\n",
    "    if return_feature:\n",
    "        rs = []\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    if parallel_process:\n",
    "        cpu_count = multiprocessing.cpu_count()\n",
    "    \n",
    "        if 'pool' not in globals():\n",
    "            pool = Pool(cpu_count-1,initializer=init)\n",
    "        \n",
    "    tokenizer = get_tokenizer()\n",
    "        \n",
    "    tot_num_no_ans = 0\n",
    "    \n",
    "    \n",
    "        \n",
    "    records = []\n",
    "    \n",
    "        \n",
    "    num_no_ans = 0\n",
    "    i += 1\n",
    "\n",
    "    jobs = ((i, c, q, is_training, return_feature) for i, c, q in gen(data_dir))\n",
    "    t0 = time()\n",
    "    results = pool.imap_unordered(work,jobs) if parallel_process else tqdm(iter(work(e, debug=debug) for e in jobs))\n",
    "    c = 0\n",
    "    for e in results:\n",
    "        for record in e:\n",
    "            if return_feature:\n",
    "                record, no_ans, r = record\n",
    "                r = tk.from_bytes(r)\n",
    "                rs.append(r)\n",
    "            else:\n",
    "                record, no_ans = record\n",
    "\n",
    "\n",
    "            records.append(record)\n",
    "\n",
    "            if no_ans:\n",
    "                num_no_ans += 1\n",
    "            c += 1\n",
    "            if c % 2500 == 0:\n",
    "                t1 = time()\n",
    "                uid, inp, start, end, p_mask, unanswerable = read(record)\n",
    "                # print(uid, tk.convert_ids_to_tokens(inp) , start, end, p_mask)\n",
    "                print('%d features (%d no ans) extracted (time: %.2f s)'%(c, num_no_ans, t1-t0))\n",
    "\n",
    "    if not return_feature:\n",
    "        random.shuffle(records)\n",
    "        with open(write_fn, 'wb') as f:\n",
    "            for record in records:\n",
    "                f.write(record)\n",
    "    tot_num_no_ans = num_no_ans\n",
    "\n",
    "    print('num has ans / num no ans : %d / %d'%(c - tot_num_no_ans, tot_num_no_ans))\n",
    "    \n",
    "    \n",
    "    if return_feature:\n",
    "        return records, rs\n",
    "    \n",
    "\n",
    "train_dir = 'train-v2.0.json'\n",
    "generate_tfrecord(train_dir, 'qa_records_squad_q', is_training=True, parallel_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
